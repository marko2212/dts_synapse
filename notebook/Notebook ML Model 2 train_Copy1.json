{
	"name": "Notebook ML Model 2 train_Copy1",
	"properties": {
		"folder": {
			"name": "Archive"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkSmall",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fc78d42a-8c9e-4bf8-abcb-750a156f23bc"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/3d1a0d9a-7e0f-434a-bb08-b3b842299587/resourceGroups/Synapse_Analytics/providers/Microsoft.Synapse/workspaces/synapsedtsws/bigDataPools/SparkSmall",
				"name": "SparkSmall",
				"type": "Spark",
				"endpoint": "https://synapsedtsws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkSmall",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import library"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import numpy as np\r\n",
					"import pandas as pd\r\n",
					"from pandas.plotting import autocorrelation_plot\r\n",
					"import matplotlib as mpl\r\n",
					"import matplotlib.pyplot as pyplot\r\n",
					"import matplotlib.pyplot as plt\r\n",
					"import seaborn as sns\r\n",
					"\r\n",
					"from datetime import datetime, timedelta\r\n",
					"import time\r\n",
					"from dateutil.parser import parse \r\n",
					"import math\r\n",
					"\r\n",
					"import warnings\r\n",
					"warnings.simplefilter(\"ignore\")\r\n",
					"\r\n",
					"from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error   # metrics\r\n",
					"from statsmodels.tsa.seasonal import seasonal_decompose\r\n",
					"from statsmodels.graphics.tsaplots import plot_pacf\r\n",
					"from statsmodels.tsa.statespace.sarimax import SARIMAX\r\n",
					"from statsmodels.tsa.ar_model import AutoReg\r\n",
					"from statsmodels.tsa.arima.model import ARIMA\r\n",
					"import pmdarima as pm\r\n",
					"\r\n",
					"from sklearn.ensemble import RandomForestRegressor\r\n",
					"from sklearn.linear_model import LinearRegression\r\n",
					"from sklearn.preprocessing import PolynomialFeatures\r\n",
					"from sklearn.tree import DecisionTreeRegressor\r\n",
					"from sklearn.model_selection import cross_val_score\r\n",
					"from sklearn.svm import SVR\r\n",
					"from sklearn.linear_model import LogisticRegression\r\n",
					"from sklearn.linear_model import Ridge\r\n",
					"from sklearn.linear_model import Lasso\r\n",
					"from sklearn import neighbors\r\n",
					"from xgboost import XGBRegressor \r\n",
					"\r\n",
					"pyplot.rcParams.update({'figure.figsize': (10, 7), 'figure.dpi': 120})\r\n",
					"print('lib are imported')"
				],
				"execution_count": 19
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Evaluating the models with data - create function"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def metric_table(test, predictions):\r\n",
					"    \r\n",
					"  mse = mean_squared_error(test, predictions)\r\n",
					"  model_r2, model_RMSE, model_MAE, model_MSE, model_Adj_R2 = [], [], [], [], []\r\n",
					"\r\n",
					"\r\n",
					"  r2 = r2_score(test, predictions)\r\n",
					"  rmse = np.sqrt(mean_squared_error(test, predictions))\r\n",
					"  mae = mean_absolute_error(test, predictions)\r\n",
					"  mse = mean_squared_error(test, predictions)\r\n",
					"  # mape = mean_absolute_percentage_error(test, predictions)\r\n",
					"\r\n",
					"  # adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)       # n - broj uzoraka, p - broj nezavis var\r\n",
					"  adj_r2 = 1-(1-r2)*(test.shape[0]-1)/(test.shape[0]-1-1)\r\n",
					"\r\n",
					"  model_r2.append(r2)\r\n",
					"  model_RMSE.append(rmse)\r\n",
					"  model_MAE.append(mae)\r\n",
					"  model_MSE.append(mse)\r\n",
					"  # model_MAPE.append(mape)\r\n",
					"  model_Adj_R2.append(adj_r2)\r\n",
					"\r\n",
					"\r\n",
					"  df_result = pd.DataFrame({\"R2\":model_r2, \"Adj_R2\": model_Adj_R2, \"RMSE\": model_RMSE, \"MAE\":model_MAE, \"MSE\": model_MSE})\r\n",
					"  df_result = round(df_result,3)\r\n",
					"  df_result = df_result.sort_values(\"R2\", ascending=False)\r\n",
					"  df_result\r\n",
					"  return df_result"
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Create function that add all missing date"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def add_all_date(df, whitout_sun=0):\r\n",
					"  rng = pd.date_range(start = df.index[0], end =df.index[-1],  freq='D')        # pocetni datum je prvi index (datum), rajnji datum je poslednji red indexa\r\n",
					"  df_date_temp = pd.DataFrame({ 'DATUM': rng}) \r\n",
					"  if whitout_sun == 1:\r\n",
					"    df_date_temp = df_date_temp[df_date_temp['DATUM'].dt.weekday != 6]          # removing sunday (6) from dummy df_date_temp\r\n",
					"  df_date_temp.set_index('DATUM', inplace=True)                                 # set column date as index\r\n",
					"  df = df[df.index.weekday != 6]                                                # removing sunday (6) from df\r\n",
					"  df = df_date_temp.join(df)                                                    # left join dataset on dummy date\r\n",
					"  df = df.fillna(0)                                                             # fill NaN values with 0\r\n",
					"  df = df.iloc[:,-1:].astype(int)                                               # convert string to int\r\n",
					"  return df\r\n",
					""
				],
				"execution_count": 21
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Create function that group date by week"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def group_by_week(df):\r\n",
					"  df['year_month_week'] = df.index.strftime('%Y_%m') + '_' + df.index.week.astype(str)      # create year_month_week from index\r\n",
					"  df_week = df.groupby('year_month_week').sum()         # group df by week\r\n",
					"  return df_week"
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Import dataset"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": true,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# credentials\r\n",
					"jdbcHostname = \"srv-db-sql-dmd.database.windows.net\"\r\n",
					"jdbcDatabase = \"db-sql-dmd\"\r\n",
					"jdbcPort = \"1433\"\r\n",
					"username = \"marko\"\r\n",
					"password = \"5Avramovic!\"\r\n",
					"jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\r\n",
					"connectionProperties = {\r\n",
					"    \"user\" : username,\r\n",
					"    \"password\" : password,\r\n",
					"    \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
					"}"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# connect to database with credentials, execut SQL \r\n",
					"pushdown_query = \"(SELECT DATUM, STAVKE_NA_KOMIS_NALOGU FROM src.komisionAggFinal ) Grupno\"\r\n",
					"df_spark = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\r\n",
					"# display(df_spark)\r\n",
					"df = df_spark.toPandas()                      # Spark df to Pandas df\r\n",
					"\r\n",
					"df.set_index('DATUM', inplace=True)\r\n",
					"df.index = pd.to_datetime(df.index)           # convert index to datetime\r\n",
					"df"
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Test-Train Split"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"n = 19\r\n",
					"\r\n",
					"df = add_all_date(df, )      # add all missing date with (df, 0) or without (df, 1) sunday\r\n",
					"train = df.iloc[:-n]\r\n",
					"test = df.iloc[-n:]\r\n",
					"\r\n",
					"test.head()"
				],
				"execution_count": 25
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Train ARIMA model and prediction"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# order = [28,1,1]\r\n",
					"# model = ARIMA(train, order=(order))\r\n",
					"# model_fit = model.fit()\r\n",
					"train"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# model = ARIMA(train, order=(p,d,q))\r\n",
					"order = [28,1,1]\r\n",
					"model = ARIMA(train, order=(order))\r\n",
					"model_fit = model.fit()\r\n",
					"prediction = model_fit.forecast(n)\r\n",
					"\r\n",
					"if not isinstance(prediction.index[0], datetime):               # ako index nije datum pretvori ga u datum\r\n",
					"  prediction = prediction.to_frame().set_index(test.index)      # move series to dataframe and set index as test datum\r\n",
					"\r\n",
					"# korekcije\r\n",
					"prediction = prediction.clip(lower=0)                           # sve negativne vrednosti svedi na 0\r\n",
					"prediction = add_all_date(prediction)                           # set sundey to 0\r\n",
					"prediction"
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Graph"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# pyplot.figure(figsize=(8, 5), dpi=80)       # adjust graph size\r\n",
					"pyplot.figure(figsize=(20, 7), dpi=80)       # adjust graph size\r\n",
					"pyplot.plot(prediction, color='red', label='prediction')\r\n",
					"pyplot.plot(test, label='real')\r\n",
					"pyplot.legend(loc=\"upper left\")\r\n",
					"pyplot.show()\r\n",
					"# print(order)\r\n",
					"metric_table(test, prediction)             # metric table\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Group by week after day prediction and plot"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"prediction_week = group_by_week(prediction)\r\n",
					"test_week = group_by_week(test)\r\n",
					"pyplot.figure(figsize=(20, 7), dpi=80)       # adjust graph size\r\n",
					"pyplot.plot(prediction_week, color='red', label='prediction')\r\n",
					"pyplot.plot(test_week, label='real')\r\n",
					"pyplot.legend(loc=\"upper left\")\r\n",
					"\r\n",
					"round(prediction_week).to_csv('prediction_week.csv', sep=',', encoding='utf-8')        # download prediction_week\r\n",
					"round(test_week).to_csv('test_week.csv', sep=',', encoding='utf-8')                    # download prediction_week\r\n",
					"print(order)\r\n",
					"metric_table(test_week, prediction_week)             # metric table"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# %%pyspark\r\n",
					"# df = spark.read.load('abfss://row@dlsynapsedts.dfs.core.windows.net/dmd/SAP/DMD_2020.csv', format='csv'\r\n",
					"# ## If header exists uncomment line below\r\n",
					"# ##, header=True\r\n",
					"# )\r\n",
					"# display(df.limit(10))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Grid search ARIMA"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# grid search ARIMA parameters for time series\r\n",
					"import warnings\r\n",
					"from math import sqrt\r\n",
					"from pandas import read_csv\r\n",
					"from statsmodels.tsa.arima.model import ARIMA\r\n",
					"from sklearn.metrics import mean_squared_error\r\n",
					"\r\n",
					"# create empty metric_grid\r\n",
					"metric_grid = pd.DataFrame(columns=['0,0'])\r\n",
					"metric_grid_average = pd.DataFrame(columns=['order'])\r\n",
					"\r\n",
					"# evaluate an ARIMA model for a given order (p,d,q)\r\n",
					"def evaluate_arima_model(X, arima_order):\r\n",
					"\t# empty metric_grid\r\n",
					"\tmetric_grid = pd.DataFrame(columns=['0,0'])\r\n",
					"\t# prepare training dataset\r\n",
					"\t# train_size = int(len(X) * 0.97)\r\n",
					"\ttrain_size = len(X)-n\r\n",
					"\ttrain, test = X[0:train_size], X[train_size:]\r\n",
					"\thistory = [x for x in train]\r\n",
					"\t# make predictions\r\n",
					"\tpredictions = list()\r\n",
					"\tfor t in range(len(test)):\r\n",
					"\t\tmodel = ARIMA(history, order=arima_order)\r\n",
					"\t\tmodel_fit = model.fit()\r\n",
					"\t\tyhat = model_fit.forecast()[0]\r\n",
					"\t\tpredictions.append(yhat)\r\n",
					"\t\thistory.append(test[t])\r\n",
					"\t# calculate out of sample error\r\n",
					"\t# rmse = sqrt(mean_squared_error(test, predictions))\r\n",
					"\tmape = mean_squared_error(test, predictions)\r\n",
					"\treturn mape\r\n",
					" \r\n",
					"# evaluate combinations of p, d and q values for an ARIMA model\r\n",
					"def evaluate_models(dataset, p_values, d_values, q_values):\r\n",
					"\tdataset = dataset.astype('float32')\r\n",
					"\tbest_score, best_cfg = float(\"inf\"), None\r\n",
					"\tfor p in p_values:\r\n",
					"\t\tfor d in d_values:\r\n",
					"\t\t\tfor q in q_values:\r\n",
					"\t\t\t\torder = (p,d,q)\r\n",
					"\t\t\t\ttry:\r\n",
					"\t\t\t\t\tmape = evaluate_arima_model(dataset, order)\r\n",
					"\t\t\t\t\tif mape < best_score:\r\n",
					"\t\t\t\t\t\tbest_score, best_cfg = mape, order\r\n",
					"\t\t\t\t\tprint('ARIMA%s MSE=%.3f' % (order,mape))\r\n",
					"          # fill metric_grid with mape result\r\n",
					"\t\t\t\t\td_q = str(d) + ',' + str(q)\r\n",
					"\t\t\t\t\tmetric_grid.loc[p, d_q] = round(mape,3)\r\n",
					"\t\t\t\texcept:\r\n",
					"\t\t\t\t\tcontinue\r\n",
					"\tprint('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\r\n",
					" "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# load dataset\r\n",
					"n = 30\r\n",
					"t_now = datetime.now()\r\n",
					"series = pd.read_csv('https://raw.githubusercontent.com/marko2212/test_data/main/DMD_stavke_komis_dataset.csv',index_col='DATUM', header=0, parse_dates=[0], dayfirst=True)\r\n",
					"# evaluate parameters\r\n",
					"# p_values = [0, 1, 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28, 29, 30]\r\n",
					"p_values = [0,1, 2]\r\n",
					"d_values = range(0, 2)\r\n",
					"q_values = range(0, 2)\r\n",
					"warnings.filterwarnings(\"ignore\")\r\n",
					"evaluate_models(series.values, p_values, d_values, q_values)\r\n",
					"\r\n",
					"print(datetime.now() - t_now)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Regression"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Features engineering"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# creating new features year, month, day\r\n",
					"df['DATUM'] = df.index\r\n",
					"features = df\r\n",
					"features['year']            = features['DATUM'].dt.year\r\n",
					"features['month']           = features['DATUM'].dt.month\r\n",
					"features['week']            = features['DATUM'].dt.week\r\n",
					"features['day']             = features['DATUM'].dt.day\r\n",
					"features['quarter']         = features['DATUM'].dt.quarter\r\n",
					"features['weekofyear']      = features['DATUM'].dt.weekofyear\r\n",
					"features['year_month']      = features['DATUM'].dt.strftime('%Y-%m')\r\n",
					"# features['YEAR_WEEK']           = features.index.strftime('%Y-w%U')         # Year week 2019-w21\r\n",
					"\r\n",
					"features['weekday']         = features['DATUM'].dt.weekday              # day in week\r\n",
					"features['days_in_month']   = features['DATUM'].dt.days_in_month        # number of day in month\r\n",
					"features['flag_is_weekend'] = np.where(features['weekday'] > 4, 1, 0)   # since none order is created on weekend this feature is usfficient\r\n",
					"features['week_start']      = features['DATUM'].dt.to_period('W').apply(lambda r: r.start_time)   # week start date\r\n",
					"\r\n",
					"features"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			}
		]
	}
}